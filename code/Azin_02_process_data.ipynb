{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b64a6d76-2eb0-430b-be96-8ab1399e933f",
   "metadata": {},
   "source": [
    "### installations\n",
    "\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge emoji -y\n",
    "```\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "```\n",
    "\n",
    "#### plotting resources\n",
    "\n",
    "- https://towardsdatascience.com/how-i-got-matplotlib-to-plot-apple-color-emojis-c983767b39e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3b8c29-0f7c-45c6-b998-ea5438486e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_in_drive = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb618c-676d-47dd-b121-c06e2f9ea6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import sys\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "# Import CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "plt.rc('axes', unicode_minus=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f626fb56-d27e-452c-8927-a606d2c71f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general helpers\n",
    "\n",
    "def save_model(mdl, models_path, mdl_name):\n",
    "  \"\"\"\n",
    "  Saves model into a pickle\n",
    "  \"\"\"\n",
    "  # models_path = '/content/drive/MyDrive/GA/models'\n",
    "  # mdl_name = 'test'\n",
    "  mdl_path = f'{models_path}/{mdl_name}.pkl'\n",
    "\n",
    "\n",
    "  with open(mdl_path, 'wb') as pickle_out:\n",
    "      pickle_out = pickle.dump(mdl, pickle_out)\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    \"\"\"\n",
    "    Taken from https://www.studytonight.com/post/scikitlearn-countvectorizer-in-nlp\n",
    "    \"\"\"\n",
    "    #lowering the text case\n",
    "    text = text.lower() \n",
    "    # remove special chars\n",
    "    text = re.sub(\"\\\\W\",\" \",text)\n",
    "    return text\n",
    "\n",
    "def get_short(text):\n",
    "    \"\"\"\n",
    "    Extracts the short string for estimators and transformers to be used in pipes.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    # https://stackoverflow.com/questions/1175208/elegant-python-function-to-convert-camelcase-to-snake-case\n",
    "    pattern = re.compile(r'(?<!^)(?=[A-Z])')\n",
    "    text = pattern.sub('_', text).lower()\n",
    "    return ''.join([t[0] for t in text.split('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada836f-05a1-4f78-a5b2-745b0fcef948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 1.202, 'neu': 0.64, 'pos': 1.158, 'compound': -0.33520000000000005}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Helpers\n",
    "import emoji\n",
    "import functools\n",
    "import operator\n",
    "import re\n",
    "\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer=SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def extract_emojis(s):\n",
    "    \"\"\"\n",
    "    Extracts emojis from text.\n",
    "    \"\"\"\n",
    "    return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'])\n",
    "\n",
    "\n",
    "def separate_emojis(em):\n",
    "    \"\"\"\n",
    "    Separates emojis from each other.\n",
    "    # https://stackoverflow.com/questions/49921720/how-to-split-emoji-from-each-other-python\n",
    "    \"\"\"\n",
    "    em_split_emoji = emoji.get_emoji_regexp().split(em)\n",
    "    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "    em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "    return ' '.join(em_split)\n",
    "\n",
    "\n",
    "def get_emoji_scores(emtext, emoji_dict): \n",
    "    items = emtext.split()\n",
    "\n",
    "    default = {'pos': 0, 'neu': 0, 'neg': 0, 'compound': 0}\n",
    "    out = {'pos': 0, 'neu': 0, 'neg': 0, 'compound': 0}\n",
    "    not_found = []\n",
    "    for item in items:\n",
    "        values = emoji_dict.get(item, default)\n",
    "        if values == default:\n",
    "            if item not in not_found:  \n",
    "                t = emoji.demojize(item)\n",
    "                text = t.replace('_', ' ').replace(':', '')\n",
    "                values = analyzer.polarity_scores(text)\n",
    "                print(item, text, values['compound'])\n",
    "                not_found.append(item)  \n",
    "        for k in default.keys():\n",
    "            out[k] +=values[k]\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_emoji_dict():\n",
    "    emojis_path = '../data/emoji_scores.csv'\n",
    "    if running_in_drive:\n",
    "        emojis_path = '/content/drive/MyDrive/GA/data/emoji_scores.csv'\n",
    "\n",
    "\n",
    "    df = pd.read_csv(emojis_path)\n",
    "\n",
    "    out = df[['emoji', 'neg', 'neu', 'pos', 'compound']]\n",
    "    out.set_index('emoji', inplace=True)\n",
    "    emoji_dict = out.to_dict(orient='index')\n",
    "    return emoji_dict\n",
    "\n",
    "\n",
    "\n",
    "def get_sentiment_score(text, emoji_dict):\n",
    "    analyzer.lexicon.update(emoji_dict)\n",
    "    vs = analyzer.polarity_scores(text)\n",
    "    emtext = separate_emojis(extract_emojis(text))\n",
    "    vs_em = get_emoji_scores(emtext, emoji_dict)\n",
    "    \n",
    "    for k, v in vs.items():\n",
    "        vs[k] += vs_em[k]\n",
    "\n",
    "    return vs\n",
    "\n",
    "\n",
    "emoji_dict = get_emoji_dict()\n",
    "\n",
    "get_sentiment_score('hello :) 😕 😕', emoji_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d010eac-bb10-4d5c-a0d4-020d6750bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Main:\n",
    "    def __init__(self, to_model=True, run_eda=True):\n",
    "        self.images_path = '../presentation/Azin_images'\n",
    "        self.models_path = '../code/Azin_models'\n",
    "        self.data_path = '../data'\n",
    "        if running_in_drive:\n",
    "            self.images_path = '/content/drive/MyDrive/GA/images'\n",
    "            self.models_path = '/content/drive/MyDrive/GA/models'\n",
    "            self.data_path = '/content/drive/MyDrive/GA/data'\n",
    "        \n",
    "        self.set_data_path()\n",
    "        self.read_data()\n",
    "        self.get_stop_words()\n",
    "        self.preprocess_data()\n",
    "\n",
    "        self.get_X_y()\n",
    "\n",
    "        self.run_train_test_split()\n",
    "\n",
    "\n",
    "        self.get_baseline_info()\n",
    "        \n",
    "        if run_eda:\n",
    "            self.do_eda()\n",
    "        self.feature_extract(transformer='CountVectorizer')\n",
    "        self.feature_extract(transformer='TfidfVectorizer')\n",
    "\n",
    "\n",
    "        \n",
    "        if to_model:\n",
    "            info_list = []\n",
    "            info = self.modeling(estimator='MultinomialNB', transformer='CountVectorizer')\n",
    "            info_list.append(info)\n",
    "\n",
    "\n",
    "            # info = self.modeling(estimator='MultinomialNB', transformer='TfidfVectorizer')\n",
    "            # info_list.append(info)\n",
    "   \n",
    "            # info = self.modeling(estimator='LogisticRegression', transformer='CountVectorizer')\n",
    "            # info_list.append(info)\n",
    "\n",
    "\n",
    "            info = self.modeling(estimator='LogisticRegression', transformer='TfidfVectorizer')\n",
    "            info_list.append(info)\n",
    "            \n",
    "            self.info_list = info_list\n",
    "            \n",
    "            self.info_df = pd.DataFrame(self.info_list).set_index(['Estimator', 'Transformer'])\n",
    "            display(self.info_df)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "    def set_data_path(self):\n",
    "        #self.path = '/content/drive/MyDrive/Colab Notebooks/anxiety_writing.csv'\n",
    "        self.path = f'{self.data_path}/anxiety_writing.csv'\n",
    "        self.sep = ','\n",
    "        self.emojis_path = f'{self.data_path}/emoji_scores.csv'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def read_data(self):\n",
    "        self.data = pd.read_csv(self.path, sep=self.sep)\n",
    "#         self.data = self.data.sample(n=1000, random_state=1)\n",
    "\n",
    "        # What is the size of our data set?\n",
    "        print('shape: ', self.data.shape)\n",
    "        self.data.fillna('', inplace=True)\n",
    "        display(self.data.head(1))\n",
    "\n",
    "\n",
    "        df = pd.read_csv(self.emojis_path)\n",
    "        self.emoji_data = df\n",
    "\n",
    "        out = df[['emoji', 'neg', 'neu', 'pos', 'compound']]\n",
    "        out.set_index('emoji', inplace=True)\n",
    "        self.emoji_dict = out.to_dict(orient='index')\n",
    "\n",
    "        display(self.emoji_data.head(1))\n",
    "\n",
    "        \n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data.\n",
    "        Clean the text.\n",
    "        Replace characters.\n",
    "        Lemmatize the sentences.\n",
    "        \"\"\"\n",
    "        self.original_text_field = 'text'\n",
    "        self.text_field = \"clean_text\"\n",
    "        self.target_field = \"label\"\n",
    "        self.main_target_field = \"subreddit\"\n",
    "        # Create label column\n",
    "\n",
    "        self.data['text'] = self.data['text'].str.lower()\n",
    "        for pattern in ['&gt', '&le', '#x200b', '&amp;#x200b', r'http\\S+']:\n",
    "            self.data['text'] = self.data['text'].str.replace(pattern, '', regex=True)\n",
    "        self.data['original_text'] = self.data['text']\n",
    "        self.data['text'] = self.data['text'].apply(lambda text: separate_emojis(text))\n",
    "        self.data['emojis'] = self.data['text'].apply(lambda s: separate_emojis(extract_emojis(s)))\n",
    "\n",
    "        self.data['sentiment'] = self.data['text'].apply(lambda s: get_sentiment_score(s, self.emoji_dict)['compound'])\n",
    "        #self.data['text'] = self.data['text'].apply(lambda s: emoji.demojize(s).replace(':', ' ').replace('_', ''))\n",
    "\n",
    "        replacements = {\"don't\": \"do not\", \"I've\": \"I have\", \n",
    "                        '&amp;amp;':'&',\n",
    "                        \"'ve\": ' have', '\\n': ' ', 'poses': 'pose', \n",
    "                        \"doesn't\": \"does not\"}\n",
    "\n",
    "        for k, v in replacements.items():\n",
    "            self.data['text'] = self.data['text'].str.replace(k, v, regex=True)\n",
    "\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        self.data['text'] = self.data['text'].apply(lambda t: ' '.join([word for word in \n",
    "                                                                        t.split(' ') if word not in self.stop_words]))\n",
    "        self.data['text'] = self.data['text'].apply(lambda t: ' '.join([lemmatizer.lemmatize(word) \n",
    "                                                                        for word in t.split(' ')]))\n",
    "\n",
    "\n",
    "        self.data['text'] = self.data['text'].str.replace('\\d+', ' ', regex=True)\n",
    "\n",
    "        self.data['clean_text'] = self.data['text'].str.replace('_', ' ')\n",
    "        self.data['subreddit'] = self.data['subreddit'].str.title()\n",
    "        self.data['label'] = self.data['subreddit'].map({'Anxiety':0, 'Writing':1})\n",
    "        self.data['subreddit'] = self.data['subreddit'].map({'Anxiety':'Anxiety', 'Writing':'No Anxiety'})\n",
    "        self.reverse_map = {0: 'Anxiety', 1: 'No Anxiety'}\n",
    "        self.display_labels=['Anxiety', 'No Anxiety']\n",
    "            \n",
    "            \n",
    "        self.data['Length'] = self.data['original_text'].apply(len)\n",
    "        self.data['Word Count']= self.data['original_text'].apply(lambda x: len(x.split()))\n",
    "        \n",
    "        self.data['Title Length'] = self.data['title'].apply(len)\n",
    "        self.data['Title Word Count']= self.data['title'].apply(lambda x: len(x.split()))\n",
    "        \n",
    "        self.data['Content Length'] = self.data['selftext'].apply(len)\n",
    "        self.data['Content Word Count']= self.data['selftext'].apply(lambda x: len(x.split()))\n",
    "\n",
    "        path_to_write = f'{self.data_path}/processed_data.csv'\n",
    "        self.data.to_csv(path_to_write, index=False)\n",
    "            \n",
    "\n",
    "    def get_X_y(self):\n",
    "        \"\"\"\n",
    "        Extract X and y\n",
    "        \"\"\"\n",
    "\n",
    "        self.X = self.data[self.text_field]\n",
    "        self.y = self.data[self.target_field]\n",
    "        \n",
    "        \n",
    "    def run_train_test_split(self, random_state=42, test_size=.3):\n",
    "        \"\"\"\n",
    "        Split the data into the training and testing sets.\n",
    "        \"\"\"\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, \n",
    "                                                            test_size=test_size, \n",
    "                                                            stratify=self.y, \n",
    "                                                            random_state=random_state)\n",
    "\n",
    "        print('\\nSize of X train: ', sys.getsizeof(self.X_train))\n",
    "\n",
    "        \n",
    "    def get_stop_words(self):\n",
    "        \"\"\"\n",
    "        Get the English stope words and add more stop words to it.\n",
    "        \"\"\"\n",
    "        words = list(CountVectorizer(stop_words='english').get_stop_words())\n",
    "        \n",
    "\n",
    "        # words.extend(['writing', 'anxiety'])\n",
    "        words.extend(['www', 'https', 'like', 'just', 'day', 'week', 'time', 'month', 'year', 'ask', 'question', 'thread', 'discus', 'discussion'])\n",
    "        words.extend(['doe', 'don', 'thing', 'really', 'ago', 've'])\n",
    "        words.extend(['x200b'])\n",
    "        self.stop_words = words\n",
    "    \n",
    "    \n",
    "    def get_baseline_info(self):\n",
    "        \"\"\"\n",
    "        Compute and display the baseline accuracy.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f'\\nBaseline Accuracy - y: {self.y.value_counts(normalize=True)[0].round(3)}')\n",
    "        print(f'\\nBaseline Accuracy - y test: {self.y_test.value_counts(normalize=True)[0].round(3)}')\n",
    "        \n",
    "        display(self.y.value_counts())\n",
    "        \n",
    "        self.baseline_accuracy = self.y_test.value_counts(normalize=True)[0].round(3)\n",
    "        \n",
    "        \n",
    "    def feature_extract(self, remove_stop_words=True, transformer='CountVectorizer', ngram_range=(1,1)):\n",
    "        \"\"\"\n",
    "        Instantiate a CountVectorizer or TfidfVectorizer.\n",
    "        Vectorize the X train and X test\n",
    "        \"\"\"\n",
    "        if remove_stop_words:\n",
    "            fe = eval(transformer)(stop_words = self.stop_words, preprocessor=custom_preprocessor, \n",
    "                                   ngram_range=ngram_range, max_df=.85)\n",
    "        else:\n",
    "            fe = eval(transformer)()\n",
    "            \n",
    "        fe.fit(self.X_train)\n",
    "        X_train_tf = fe.transform(self.X_train)\n",
    "\n",
    "            \n",
    "        # Transform test\n",
    "        X_test_tf = fe.transform(self.X_test)\n",
    "        # cvec.vocabulary_\n",
    "\n",
    "        self.X_train_df = pd.DataFrame(X_train_tf.todense(), columns=fe.get_feature_names())\n",
    "        self.X_test_df = pd.DataFrame(X_test_tf.todense(), columns=fe.get_feature_names())\n",
    "\n",
    "        \n",
    "\n",
    "    def plot_distributions(self):\n",
    "        \"\"\"\n",
    "        Plot histograms for the distributions of character length and word length\n",
    "        \"\"\"\n",
    "        \n",
    "        data = self.data\n",
    "        \n",
    "        data = data[data['Content Length']>0]\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(16, 6));\n",
    "        g0 = sns.histplot(data, x='Length', hue=self.main_target_field, ax = ax[0], kde=True, legend=False, alpha=.45);\n",
    "        g1 = sns.histplot(data, x='Word Count', hue=self.main_target_field, ax = ax[1], kde=True, alpha=.45);\n",
    "        g0.set(title='Character Length Distributions')\n",
    "        g1.set(title='Word Count Distributions')\n",
    "        ax[0].set_xlim(None, 4000)\n",
    "        ax[1].set_xlim(None, 600)\n",
    "        ax[1].set_ylabel('');\n",
    "        sns.despine(top=True);\n",
    "        fig.suptitle('Title+Content', fontsize=20);\n",
    "        plt.savefig(f'{self.images_path}/character_word_count_dist_title_content.png', bbox_inches='tight', dpi=300)\n",
    "        \n",
    "        \n",
    "        fig, ax = plt.subplots(1, 2, figsize=(16, 6));\n",
    "        g0 = sns.histplot(data, x='Title Length', hue=self.main_target_field, ax = ax[0], kde=True, legend=False, alpha=.45);\n",
    "        g1 = sns.histplot(data, x='Title Word Count', hue=self.main_target_field, ax = ax[1], kde=True, alpha=.45);\n",
    "        g0.set(title='Character Length Distributions')\n",
    "        g1.set(title='Word Count Distributions')\n",
    "        ax[1].set_ylabel('');\n",
    "        sns.despine(top=True);\n",
    "        fig.suptitle('Title', fontsize=20);\n",
    "        plt.savefig(f'{self.images_path}/character_word_count_dist_title.png', bbox_inches='tight', dpi=300)\n",
    "        \n",
    "\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(16, 6));\n",
    "        g0 = sns.histplot(data, x='Content Length', hue=self.main_target_field, ax = ax[0], kde=True, legend=False, alpha=.45);\n",
    "        g1 = sns.histplot(data, x='Content Word Count', hue=self.main_target_field, ax = ax[1], kde=True, alpha=.45);\n",
    "        g0.set(title='Character Length Distributions')\n",
    "        g1.set(title='Word Count Distributions')\n",
    "        ax[0].set_xlim(None, 4000)\n",
    "        ax[1].set_xlim(None, 600)\n",
    "        ax[1].set_ylabel('');\n",
    "        sns.despine(top=True);\n",
    "        fig.suptitle('Content', fontsize=20);\n",
    "        plt.savefig(f'{self.images_path}/character_word_count_dist_content.png', bbox_inches='tight', dpi=300)\n",
    "        \n",
    "        \n",
    "    def do_eda(self):\n",
    "        \"\"\"\n",
    "        Plot Distributions, bar charts for frequently occurring words.\n",
    "        \"\"\"\n",
    "        self.plot_distributions()\n",
    "        self.get_top_occurring('CountVectorizer', count=15, ngram_range = (3,3))\n",
    "        self.get_top_occurring('TfidfVectorizer', count=15, ngram_range = (3,3))\n",
    "        self.get_top_occurring('CountVectorizer', count=15, ngram_range = (2,2))\n",
    "        self.get_top_occurring('TfidfVectorizer', count=15, ngram_range = (2,2))\n",
    "        self.get_top_occurring('CountVectorizer', count=15, ngram_range = (1,1))\n",
    "        self.get_top_occurring('TfidfVectorizer', count=15, ngram_range = (1,1))\n",
    "        self.get_top_occurring_separated(count=15, ngram_range = (3,3))\n",
    "        self.get_top_occurring_separated(count=15, ngram_range = (2,2))\n",
    "        self.get_top_occurring_separated(count=15, ngram_range = (1,1))\n",
    "        self.get_hashtags_separated(count=10)\n",
    "        self.get_emojis_separated(count=10)\n",
    "        \n",
    "    def get_hashtags_separated(self, count=10):\n",
    "        \"\"\"\n",
    "        Extract the hashtags for each subreddit.\n",
    "        \"\"\"\n",
    "        data0 = self.data[self.data[self.target_field]==0]\n",
    "        data1 = self.data[self.data[self.target_field]==1]\n",
    "        \n",
    "        cv = CountVectorizer(stop_words= self.stop_words, ngram_range=(1,1), token_pattern=r'\\b\\w\\w+\\b|(?<!\\w)@\\w+|(?<!\\w)#\\w+')\n",
    "\n",
    "        vect_df0 = pd.DataFrame(cv.fit_transform(data0['original_text']).todense(), columns = cv.get_feature_names())\n",
    "        vect_df1 = pd.DataFrame(cv.fit_transform(data1['original_text']).todense(), columns = cv.get_feature_names())\n",
    "        \n",
    "        self.one_gram_features_0 = vect_df0.columns.tolist()\n",
    "        self.one_gram_features_1 = vect_df1.columns.tolist()\n",
    "\n",
    "        self.hashtags0 = pd.DataFrame(vect_df0.sum()\n",
    "                                 .sort_values(ascending=False),\n",
    "                                 columns=['count'],\n",
    "                                )\n",
    "        self.hashtags1 = pd.DataFrame(vect_df1.sum()\n",
    "                                 .sort_values(ascending=False),\n",
    "                                 columns=['count'],\n",
    "                                )\n",
    "        \n",
    "\n",
    "        self.hashtags0.index.name = 'word'\n",
    "        self.hashtags0.reset_index(inplace=True)\n",
    "        self.hashtags1.index.name = 'word'\n",
    "        self.hashtags1.reset_index(inplace=True)\n",
    "        \n",
    "        self.hashtags0 = self.hashtags0[self.hashtags0['word'].str.startswith('#')]\n",
    "        self.hashtags1 = self.hashtags1[self.hashtags1['word'].str.startswith('#')]\n",
    "        \n",
    "        self.hashtags_0 = [x for x in self.one_gram_features_0 if x.startswith('#')]\n",
    "        self.hashtags_1 = [x for x in self.one_gram_features_1 if x.startswith('#')]\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        g = sns.barplot(data=self.hashtags0.head(count), y='word', x='count', ax=ax[0], \n",
    "                        edgecolor=sns.color_palette(\"flare\", 10),\n",
    "                        facecolor=(0, 0, 0, 0));\n",
    "        g.set_title(f'{self.reverse_map[0]}');\n",
    "        g = sns.barplot(data=self.hashtags1.head(count), y='word', x='count', ax=ax[1], \n",
    "                        edgecolor=sns.color_palette(\"crest\", count),\n",
    "                        facecolor=(0, 0, 0, 0));\n",
    "        ax[1].set_ylabel('');\n",
    "        ax[0].set_ylabel('');\n",
    "        ax[0].set_xlabel('');\n",
    "        ax[1].set_xlabel('');\n",
    "        g.set_title(f'{self.reverse_map[1]}');\n",
    "        sns.despine(top=True);\n",
    "        ax[0].tick_params(left=False, right=False, bottom=False);\n",
    "        ax[1].tick_params(left=False, right=False, bottom=False);\n",
    "        ax[0].tick_params(axis=\"y\",direction=\"in\", pad=-200)\n",
    "        ax[1].tick_params(axis=\"y\",direction=\"in\", pad=-200)\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        display(self.hashtags0)\n",
    "        display(self.hashtags1)\n",
    "        plt.savefig(f'{self.images_path}/hashtags.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "        \n",
    "    def get_emojis_separated(self, count=10):\n",
    "        \"\"\"\n",
    "        Extract the hashtags for each subreddit.\n",
    "        \"\"\"\n",
    "        data0 = self.data[self.data[self.target_field]==0]\n",
    "        data1 = self.data[self.data[self.target_field]==1]\n",
    "        \n",
    "        cv = CountVectorizer(stop_words= self.stop_words, ngram_range=(1,1), analyzer = 'char', binary = True)\n",
    "\n",
    "        vect_df0 = pd.DataFrame(cv.fit_transform(data0['emojis']).todense(), columns = cv.get_feature_names())\n",
    "        vect_df1 = pd.DataFrame(cv.fit_transform(data1['emojis']).todense(), columns = cv.get_feature_names())\n",
    "        \n",
    "        self.one_gram_features_emojis_0 = vect_df0.columns.tolist()\n",
    "        self.one_gram_features_emojis_1 = vect_df1.columns.tolist()\n",
    "\n",
    "        self.emojis0 = pd.DataFrame(vect_df0.sum()\n",
    "                                 .sort_values(ascending=False),\n",
    "                                 columns=['count'],\n",
    "                                )\n",
    "        \n",
    "        \n",
    "        self.emojis1 = pd.DataFrame(vect_df1.sum()\n",
    "                                 .sort_values(ascending=False),\n",
    "                                 columns=['count'],\n",
    "                                )\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        self.emojis0.index.name = 'word'\n",
    "        self.emojis0.reset_index(inplace=True)\n",
    "        self.emojis1.index.name = 'word'\n",
    "        self.emojis1.reset_index(inplace=True)\n",
    "        \n",
    "        self.emojis0 = self.emojis0[self.emojis0['word']!=' ']\n",
    "        self.emojis1 = self.emojis1[self.emojis1['word']!=' ']\n",
    "\n",
    "        \n",
    "        \n",
    "        self.emojis_0 = [x for x in self.one_gram_features_emojis_0 if x!=None ]\n",
    "        self.emojis_1 = [x for x in self.one_gram_features_emojis_1 if x!=None ]\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        g = sns.barplot(data=self.emojis0.head(count), y='word', x='count', ax=ax[0], \n",
    "                        edgecolor=sns.color_palette(\"flare\", 10),\n",
    "                        facecolor=(0, 0, 0, 0));\n",
    "        g.set_title(f'{self.reverse_map[0]}');\n",
    "        g = sns.barplot(data=self.emojis1.head(count), y='word', x='count', ax=ax[1], \n",
    "                        edgecolor=sns.color_palette(\"crest\", count),\n",
    "                        facecolor=(0, 0, 0, 0));\n",
    "        ax[1].set_ylabel('');\n",
    "        ax[0].set_ylabel('');\n",
    "        ax[0].set_xlabel('');\n",
    "        ax[1].set_xlabel('');\n",
    "        g.set_title(f'{self.reverse_map[1]}');\n",
    "        sns.despine(top=True);\n",
    "        ax[0].tick_params(left=False, right=False, bottom=False);\n",
    "        ax[1].tick_params(left=False, right=False, bottom=False);\n",
    "        ax[0].tick_params(axis=\"y\",direction=\"in\", pad=-200)\n",
    "        ax[1].tick_params(axis=\"y\",direction=\"in\", pad=-200)\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        display(self.emojis0)\n",
    "        display(self.emojis1)\n",
    "        plt.savefig(f'{self.images_path}/emojis.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "        \n",
    "    def get_top_occurring_separated(self, count=10, ngram_range = (1,2)):\n",
    "        \"\"\"\n",
    "        Extract the commonly occurring words for each subreddit\n",
    "        \"\"\"\n",
    "        data0 = self.data[self.data[self.target_field]==0]\n",
    "        data1 = self.data[self.data[self.target_field]==1]\n",
    "        \n",
    "\n",
    "        cv = CountVectorizer(stop_words= self.stop_words, ngram_range=ngram_range, token_pattern=r'\\b\\w\\w+\\b|(?<!\\w)@\\w+|(?<!\\w)#\\w+')\n",
    "        \n",
    "         \n",
    "\n",
    "        vect_df0 = pd.DataFrame(cv.fit_transform(data0[self.original_text_field]).todense(), columns = cv.get_feature_names())\n",
    "        vect_df1 = pd.DataFrame(cv.fit_transform(data1[self.original_text_field]).todense(), columns = cv.get_feature_names())\n",
    "        \n",
    "\n",
    "\n",
    "        self.top_occurring0 = pd.DataFrame(vect_df0.sum()\n",
    "                                 .sort_values(ascending=False),\n",
    "                                 columns=['count'],\n",
    "                                )\n",
    "        self.top_occurring1 = pd.DataFrame(vect_df1.sum()\n",
    "                                 .sort_values(ascending=False),\n",
    "                                 columns=['count'],\n",
    "                                )\n",
    "        self.top_occurring0.index.name = 'word'\n",
    "        self.top_occurring0.reset_index(inplace=True)\n",
    "        self.top_occurring1.index.name = 'word'\n",
    "        self.top_occurring1.reset_index(inplace=True)\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        g = sns.barplot(data=self.top_occurring0.head(count), y='word', x='count', ax=ax[0], \n",
    "                        edgecolor=sns.color_palette(\"flare\", 10),\n",
    "                        facecolor=(0, 0, 0, 0));\n",
    "        g.set_title(f'{self.reverse_map[0]}');\n",
    "        g = sns.barplot(data=self.top_occurring1.head(count), y='word', x='count', ax=ax[1], \n",
    "                        edgecolor=sns.color_palette(\"crest\", count),\n",
    "                        facecolor=(0, 0, 0, 0));\n",
    "        ax[1].set_ylabel('');\n",
    "        ax[0].set_ylabel('');\n",
    "        g.set_title(f'{self.reverse_map[1]}');\n",
    "        sns.despine(top=True);\n",
    "        ax[0].tick_params(left=False, right=False, bottom=False);\n",
    "        ax[1].tick_params(left=False, right=False, bottom=False);\n",
    "        if ngram_range[1]==3:\n",
    "            pad = -240\n",
    "        else:\n",
    "            pad = -150\n",
    "        ax[0].tick_params(axis=\"y\",direction=\"in\", pad=pad)\n",
    "        ax[1].tick_params(axis=\"y\",direction=\"in\", pad=pad)\n",
    "        fig.suptitle(f'{ngram_range[1]}-gram Words', fontsize=30);\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        plt.savefig(f'{self.images_path}/top_occurring_ngram_{ngram_range[0]}_{ngram_range[1]}.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "    def get_top_occurring(self, transformer, ngram_range=(1,2), count=20):\n",
    "        \"\"\"\n",
    "        Get the top occurring words in all the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        cv = CountVectorizer(stop_words= self.stop_words, ngram_range=ngram_range, \n",
    "                             token_pattern=r'\\b\\w\\w+\\b|(?<!\\w)@\\w+|(?<!\\w)#\\w+')\n",
    "        \n",
    "        vect_df = pd.DataFrame(cv.fit_transform(self.data[self.original_text_field]).todense(), columns = cv.get_feature_names())\n",
    "        top_occurring = pd.DataFrame(vect_df.sum()\n",
    "                                 .sort_values(ascending=False),\n",
    "                                 columns=['count'],\n",
    "                                )\n",
    "        top_occurring.index.name = 'word'\n",
    "        top_occurring.reset_index(inplace=True)\n",
    "        self.top_occurring = top_occurring\n",
    "        #display(top_occurring)\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        g = sns.barplot(data=self.top_occurring.head(count), y='word', x='count',\n",
    "                       edgecolor=sns.color_palette(\"colorblind\", count),\n",
    "                        facecolor=(0, 0, 0, 0));\n",
    "        \n",
    "#         fig.suptitle(f'{ngram_range[1]}-gram Words', fontsize=30);\n",
    "        g.set_title(f'{ngram_range[1]}-gram Words');\n",
    "        sns.despine(top=True);\n",
    "        g.set_ylabel('');\n",
    "        g.tick_params(left=False, right=False, bottom=False);\n",
    "        if ngram_range[1]==3:\n",
    "            pad = -270\n",
    "        else:\n",
    "            pad = -200\n",
    "        g.tick_params(axis=\"y\",direction=\"in\", pad=pad)\n",
    "        plt.savefig(f'{self.images_path}/all_top_occurring_{transformer}_ngram_{ngram_range[0]}_{ngram_range[1]}.png', \n",
    "                    bbox_inches='tight', dpi=300)\n",
    "        \n",
    "    def do_grid_search(self, mdl, param_grid, cv = 5, results={}):\n",
    "\n",
    "        # Instantiate GridSearchCV.\n",
    "        gs = GridSearchCV(\n",
    "            # what object are we optimizing?\n",
    "            estimator = mdl,\n",
    "            # what parameters values are we searching?\n",
    "            param_grid = param_grid,\n",
    "            # 5-fold cross-validation.\n",
    "            cv = cv,\n",
    "            n_jobs = -1\n",
    "        )\n",
    "\n",
    "        # Fit GridSearch to training data.\n",
    "        gs.fit(self.X_train, self.y_train)\n",
    "\n",
    "        # What's the best score?\n",
    "        best_score = gs.best_score_\n",
    "\n",
    "        # What are the best hyperparameters?\n",
    "        best_hyperparameters = gs.best_params_\n",
    "\n",
    "        # Score model on training set.\n",
    "        train_score = gs.score(self.X_train, self.y_train)\n",
    "\n",
    "        # Score model on testing set.\n",
    "        test_score = gs.score(self.X_test, self.y_test)\n",
    "\n",
    "\n",
    "        print('\\n\\tGrid Search Results')\n",
    "        print('\\t\\tBest Parameters')\n",
    "        for k, v in best_hyperparameters.items():\n",
    "            print(f'\\t\\t{k} : {v}')\n",
    "\n",
    "        print('\\n\\t\\tBest Score ', best_score.round(3))\n",
    "        print('\\t\\tTrain Score: ', train_score.round(3))\n",
    "        print('\\t\\tTest Score: ', test_score.round(3))\n",
    "        \n",
    "        results.update({\n",
    "            'Grid Search Given Parameters': param_grid,\n",
    "            'Grid Search Best Score': best_score.round(3),\n",
    "            'Grid Search Train Score': train_score.round(3),\n",
    "            'Grid Search Test Score': test_score.round(3),\n",
    "            'Grid Search Best Hyperparameters': best_hyperparameters\n",
    "            })\n",
    "        return gs, results\n",
    "    \n",
    "    \n",
    "    def make_confusion_matrix(self, mdl, results={}):\n",
    "        \"\"\"\n",
    "        Plot the confusion matrix.\n",
    "        \"\"\"\n",
    "        # Get predictions\n",
    "        self.preds = mdl.predict(self.X_test)\n",
    "\n",
    "        # Save confusion matrix values\n",
    "        tn, fp, fn, tp = confusion_matrix(self.y_test, self.preds).ravel()\n",
    "\n",
    "\n",
    "        # View confusion matrix\n",
    "        disp = plot_confusion_matrix(mdl, self.X_test, self.y_test, cmap='copper', \n",
    "                                     values_format='d', display_labels=self.display_labels);\n",
    "\n",
    "\n",
    "        # Calculate the specificity (TRUE NEG RATE)\n",
    "        spec = tn/(tn + fp)\n",
    "\n",
    "        # calculate precision\n",
    "        prec = tp / (tp + fp)\n",
    "        f1 = 2 * tp / (2*tp+fp+fn)\n",
    "        \n",
    "        sens = tp / (tp + fn)\n",
    "        \n",
    "        acc = (tn + tp) / (tn + fp + fn + tp)\n",
    "\n",
    "        print('\\tF1 - score: ', f1.round(3))\n",
    "        print('\\tRecall (Sensitivity): ', sens.round(3))\n",
    "        print('\\tSpecificity (True Negative Rate): ', spec.round(3))\n",
    "        print('\\tPrecision: ', prec.round(3))\n",
    "        print('\\tAccuracy: ', acc.round(3))\n",
    "        results.update({\n",
    "            'F1 - score': f1.round(3),\n",
    "            'Recall (Sensitivity)': sens.round(3),\n",
    "            'Specificity (True Negative Rate)': spec.round(3),\n",
    "            'Precision': prec.round(3),\n",
    "            'Accuracy': acc.round(3),\n",
    "            \n",
    "        })\n",
    "#         plt.text(3, -1.5, f'Accuracy: {acc.round(2)*100:.0f}%', fontsize=20)\n",
    "#         plt.text(3, -1, f'Precision: {prec.round(2)*100:.0f}%', fontsize=20)\n",
    "#         plt.text(3, -.5, f'F1 - score: {f1.round(2)*100:.0f}%', fontsize=20)\n",
    "#         plt.text(3, 0, f'Recall (Sensitivity): {sens.round(2)*100:.0f}%', fontsize=20)\n",
    "#         plt.text(3, .5, f'Specificity (True Negative Rate): {spec.round(2)*100:.0f}%', fontsize=20)\n",
    "        \n",
    "#         indent = .5\n",
    "#         for k, v in results['Grid Search Best Hyperparameters'].items():\n",
    "#             indent+=.5\n",
    "#             plt.text(3, indent, f\"{k.split('__')[1]}: {v}\", fontsize=20)\n",
    "        return disp, results\n",
    "    \n",
    "    def modeling(self, \n",
    "                 estimator='MultinomialNB',\n",
    "                 transformer='CountVectorizer',\n",
    "                 grid_search=True,\n",
    "                 cv = 5):\n",
    "        \"\"\"\n",
    "        Model the data with the given parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        # Let's set a pipeline up with two stages:\n",
    "        # 1. CountVectorizer (transformer) or tf-idf vectorizer (transformer)\n",
    "        # 2. Multinomial Naive Bayes (estimator)\n",
    "\n",
    "        transformer_short = get_short(transformer)\n",
    "        estimator_short = get_short(estimator)\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            (transformer_short, eval(transformer)(stop_words=self.stop_words, ngram_range=(1,2))),\n",
    "            (estimator_short, eval(estimator)())    \n",
    "        ])\n",
    "\n",
    "        # Estimate how your model will perform on unseen data\n",
    "        cv_score = cross_val_score(pipe, self.X_train, self.y_train, cv=cv).mean()\n",
    "\n",
    "        # Fit your model\n",
    "        pipe.fit(self.X_train, self.y_train)\n",
    "\n",
    "        # Training score\n",
    "        train_score = pipe.score(self.X_train, self.y_train)\n",
    "        # Test score\n",
    "        test_score = pipe.score(self.X_test, self.y_test)\n",
    "                        \n",
    "        info = {'Transformer': transformer,\n",
    "                'Estimator': estimator,\n",
    "                'Basic Validation Score': cv_score.round(3),\n",
    "                'Basic Train Score': train_score.round(3),\n",
    "                'Basic Test Score': test_score.round(3)}\n",
    "                        \n",
    "\n",
    "        print(f'\\nResults for transformer: \"{transformer}\" & estimator: \"{estimator}\"')\n",
    "        print('\\tValidation Score: ', cv_score.round(3))\n",
    "        print('\\tTrain Score: ', train_score.round(3))\n",
    "        print('\\tTest Score: ', test_score.round(3))\n",
    "\n",
    "        mdl = pipe\n",
    "        to_save_mdl = pipe\n",
    "        \n",
    "\n",
    "        if grid_search:\n",
    "            if transformer == 'CountVectorizer':\n",
    "                # Search over the following values of hyperparameters:\n",
    "                # Maximum number of features fit: 2000, 3000, 4000, 5000\n",
    "                # Minimum number of documents needed to include token: 2, 3\n",
    "                # Maximum number of documents needed to include token: 90%, 95%\n",
    "                # Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n",
    "                param_grid = {\n",
    "                f'{transformer_short}__min_df': [2, 3, 4],\n",
    "                f'{transformer_short}__max_df': [0.92, 0.95],\n",
    "                f'{transformer_short}__ngram_range': [(1,1), (1,2), (2, 3)]\n",
    "                }\n",
    "                \n",
    "                if estimator == 'DecisionTreeClassifier':\n",
    "                    param_grid.update({\n",
    "                        f'{estimator_short}__max_depth': [5, 10, 20, 30],\n",
    "                        f'{estimator_short}__min_samples_split': [5, 10, 15, 20, 30],\n",
    "                        f'{estimator_short}__min_samples_leaf': [5, 10],\n",
    "                    })\n",
    "                elif estimator == 'RandomForestClassifier':\n",
    "                    param_grid.update({\n",
    "                        f'{estimator_short}__n_estimators': [100, 150, 200],\n",
    "                        f'{estimator_short}__max_depth': [None, 1, 2, 3, 4, 5],\n",
    "                        f'{estimator_short}__max_features': ['sqrt', .5]\n",
    "                    })\n",
    "                else:\n",
    "                    param_grid.update({\n",
    "                        f'{transformer_short}__max_features': [2000, 3000, 4000, 5000],\n",
    "                    })\n",
    "                \n",
    "            elif transformer == 'TfidfVectorizer':\n",
    "                # Search over the following values of hyperparameters:\n",
    "                # Maximum number of features fit: 2000, 3000, 4000, 5000\n",
    "                # No stop words and english stop words\n",
    "                # Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "                \n",
    "                param_grid = {\n",
    "                    f'{transformer_short}__ngram_range': [(1,1), (1,2), (1, 3)],\n",
    "                    f'{transformer_short}__max_features': [500, 1000, 2000, 3000],\n",
    "                }\n",
    "                \n",
    "                if estimator == 'DecisionTreeClassifier':\n",
    "                    param_grid.update({\n",
    "                        f'{estimator_short}__max_depth': [5, 10, 20, 30],\n",
    "                        f'{estimator_short}__min_samples_split': [5, 10, 15, 20, 30],\n",
    "                        f'{estimator_short}__min_samples_leaf': [5, 10],\n",
    "                    })\n",
    "                elif estimator == 'RandomForestClassifier':\n",
    "                    param_grid.update({\n",
    "                        'rf__n_estimators': [100],\n",
    "                        'rf__max_depth': [None, 5],\n",
    "#                         'rf__max_features': ['sqrt', .5]\n",
    "                    })\n",
    "                # else:\n",
    "                #     param_grid.update({\n",
    "                #         f'{transformer_short}__max_features': [2000, 3000, 4000, 5000],\n",
    "                #     })\n",
    "            \n",
    "\n",
    "            gs, info = self.do_grid_search(mdl, param_grid, cv = 5, results=info)\n",
    "            mdl = gs.best_estimator_ \n",
    "            to_save_mdl = gs\n",
    "\n",
    "        self.preds = mdl.predict(self.X_test)\n",
    "        feature_importances = self.get_feature_importances(mdl, transformer, estimator)\n",
    "        coefficients = self.get_coefficients(mdl, transformer, estimator)\n",
    "        disp, info = self.make_confusion_matrix(mdl, results=info)\n",
    "        if disp is not None:\n",
    "          disp.ax_.set_title(f'Confusion Matrix\\nTransformer: \"{transformer}\"\\nEstimator: \"{estimator}\"')\n",
    "          plt.savefig(f'{self.images_path}/confusion_matrix_{transformer}_{estimator}.png', bbox_inches='tight', dpi=300)\n",
    "        info.update({'mdl': mdl})\n",
    "        info.update({'to_save_mdl': to_save_mdl})\n",
    "        save_model(to_save_mdl, self.models_path, f'grid_{transformer}_{estimator}')\n",
    "        save_model(mdl, self.models_path, f'mdl_{transformer}_{estimator}')\n",
    "        return info\n",
    "\n",
    "\n",
    "    def get_coefficients(self, mdl, transformer, estimator):\n",
    "        print(transformer)\n",
    "        print(estimator)\n",
    "        \"\"\"\n",
    "        Extract the feature importances from the resulting model.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "          coefficients = mdl.named_steps[get_short(estimator)].coef_[0]\n",
    "          vec = mdl.named_steps[get_short(transformer)]\n",
    "          col_names = vec.get_feature_names()\n",
    "\n",
    "\n",
    "          fi = pd.DataFrame(\n",
    "                      pd.Series(coefficients, col_names, name='Coefficients')\n",
    "                              .sort_values(ascending=False)).head(30)\n",
    "\n",
    "\n",
    "          fi.index.name='Word'\n",
    "          fi.reset_index(inplace=True)\n",
    "          self.fi = fi\n",
    "          plt.figure(figsize=(15, 10))\n",
    "          g = sns.barplot(data=fi, y='Word', x='Coefficients', \n",
    "                  edgecolor=sns.color_palette(\"coolwarm\", fi.shape[0]),\n",
    "                  facecolor=(0, 0, 0, 0));\n",
    "          sns.despine(top=True);\n",
    "          g.set_ylabel('');\n",
    "          g.tick_params(left=False, right=False, bottom=False);\n",
    "          g.set_title(f'Transformer: \"{transformer}\"\\nEstimator: \"{estimator}\"', fontsize = 30)\n",
    "          plt.savefig(f'{self.images_path}/coefficients_{transformer}_{estimator}.png', bbox_inches='tight', dpi=300)\n",
    "          return coefficients\n",
    "        except Exception as e:\n",
    "          print(e)\n",
    "          return\n",
    "\n",
    "\n",
    "\n",
    "    def get_important_features_mnb(self, mdl, transformer, estimator):\n",
    "        print(transformer)\n",
    "        print(estimator)\n",
    "\n",
    "        if estimator != 'MultinomialNB':\n",
    "          return\n",
    "\n",
    "        \"\"\"\n",
    "        Extract the feature importances from the resulting model.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "          NB_optimal = mdl.named_steps[get_short(estimator)]\n",
    "          vec = mdl.named_steps[get_short(transformer)]\n",
    "          col_names = vec.get_feature_names()\n",
    "\n",
    "          neg_class_prob_sorted = NB_optimal.feature_log_prob_[0, :].argsort()[::-1]\n",
    "          pos_class_prob_sorted = NB_optimal.feature_log_prob_[1, :].argsort()[::-1]\n",
    "\n",
    "          print(np.take(col_names, neg_class_prob_sorted[:10]))\n",
    "          print(np.take(col_names, pos_class_prob_sorted[:10]))\n",
    "\n",
    "          fi_neg = pd.DataFrame(\n",
    "                      pd.Series(NB_optimal.feature_log_prob_[0, :], col_names, name='Feature Log Probability')\n",
    "                              .sort_values(ascending=False)).head(30)\n",
    "\n",
    "          fi_pos = pd.DataFrame(\n",
    "                      pd.Series(NB_optimal.feature_log_prob_[1, :], col_names, name='Feature Log Probability')\n",
    "                              .sort_values(ascending=False)).head(30)\n",
    "\n",
    "\n",
    "          fi = fi_neg\n",
    "          fi.index.name='Word'\n",
    "          fi.reset_index(inplace=True)\n",
    "          self.fi = fi\n",
    "          plt.figure(figsize=(15, 10))\n",
    "          g = sns.barplot(data=fi, y='Word', x='Feature Log Probability', \n",
    "                  edgecolor=sns.color_palette(\"coolwarm\", fi.shape[0]),\n",
    "                  facecolor=(0, 0, 0, 0));\n",
    "          sns.despine(top=True);\n",
    "          g.set_ylabel('');\n",
    "          g.tick_params(left=False, right=False, bottom=False);\n",
    "          g.set_title(f'Transformer: \"{transformer}\"\\nEstimator: \"{estimator}\"', fontsize = 30)\n",
    "          plt.savefig(f'{self.images_path}/mnb_importance_{transformer}_{estimator}.png', bbox_inches='tight', dpi=300)\n",
    "          return NB_optimal.feature_log_prob_[1, :]\n",
    "        except Exception as e:\n",
    "          print(e)\n",
    "          return\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_feature_importances(self, mdl, transformer, estimator):\n",
    "        print(transformer)\n",
    "        print(estimator)\n",
    "        \"\"\"\n",
    "        Extract the feature importances from the resulting model.\n",
    "        \"\"\"\n",
    "\n",
    "        if estimator == 'MultinomialNB':\n",
    "          return self.get_important_features_mnb(mdl, transformer, estimator)\n",
    "\n",
    "        feature_importances = None\n",
    "        try:\n",
    "            mdl._final_estimator\n",
    "            try:\n",
    "                feature_importances = mdl._final_estimator.feature_importances_\n",
    "                self.feature_extract(transformer=transformer, ngram_range=(1,1))\n",
    "                vec = mdl.named_steps[get_short(transformer)]\n",
    "                col_names = vec.get_feature_names()\n",
    "\n",
    "                fi = pd.DataFrame(\n",
    "                    pd.Series(feature_importances, col_names, name='Feature Importance')\n",
    "                            .sort_values(ascending=False)).head(30)\n",
    "                fi.index.name='Word'\n",
    "                fi.reset_index(inplace=True)\n",
    "                self.fi = fi\n",
    "                plt.figure(figsize=(15, 10))\n",
    "                g = sns.barplot(data=fi, y='Word', x='Feature Importance', \n",
    "                        edgecolor=sns.color_palette(\"coolwarm\", fi.shape[0]),\n",
    "                        facecolor=(0, 0, 0, 0));\n",
    "                sns.despine(top=True);\n",
    "                g.set_ylabel('');\n",
    "                g.tick_params(left=False, right=False, bottom=False);\n",
    "                g.set_title(f'Transformer: \"{transformer}\"\\nEstimator: \"{estimator}\"', fontsize = 30)\n",
    "                plt.savefig(f'{self.images_path}/feature_importance_{transformer}_{estimator}.png', bbox_inches='tight', dpi=300)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        return feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa07f4-2c40-410e-ac71-40a0b74facff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (1000, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>4xhightimes</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>Long story short I took a 3 day thc break, smo...</td>\n",
       "      <td>Weed/cart induced anxiety attack?</td>\n",
       "      <td>1606616331</td>\n",
       "      <td>Weed/cart induced anxiety attack? Long story ...</td>\n",
       "      <td>Weed/cart induced anxiety attack? Long story ...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author link_flair_text  num_comments  \\\n",
       "1483  4xhightimes                             2   \n",
       "\n",
       "                                               selftext  \\\n",
       "1483  Long story short I took a 3 day thc break, smo...   \n",
       "\n",
       "                                  title  created_utc  \\\n",
       "1483  Weed/cart induced anxiety attack?   1606616331   \n",
       "\n",
       "                                                   text  \\\n",
       "1483   Weed/cart induced anxiety attack? Long story ...   \n",
       "\n",
       "                                             clean_text subreddit  \n",
       "1483   Weed/cart induced anxiety attack? Long story ...   Anxiety  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Char</th>\n",
       "      <th>Unicode</th>\n",
       "      <th>Occurrences [5-max]</th>\n",
       "      <th>Position [0-1]</th>\n",
       "      <th>Neg [0-1]</th>\n",
       "      <th>Neut [0-1]</th>\n",
       "      <th>Pos [0-1]</th>\n",
       "      <th>Sentiment score [-1-+1]</th>\n",
       "      <th>Sentiment bar (c.i. 95%)</th>\n",
       "      <th>name</th>\n",
       "      <th>block</th>\n",
       "      <th>demojized</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>emoji</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>😂</td>\n",
       "      <td>0x1f602</td>\n",
       "      <td>14622</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FACE WITH TEARS OF JOY</td>\n",
       "      <td>Emoticons</td>\n",
       "      <td>:face_with_tears_of_joy:</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.468</td>\n",
       "      <td>😂</td>\n",
       "      <td>0.221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Char  Unicode  Occurrences [5-max]  Position [0-1]  Neg [0-1]  Neut [0-1]  \\\n",
       "0    😂  0x1f602                14622           0.805      0.247       0.285   \n",
       "\n",
       "   Pos [0-1]  Sentiment score [-1-+1]  Sentiment bar (c.i. 95%)  \\\n",
       "0      0.468                    0.221                       NaN   \n",
       "\n",
       "                     name      block                 demojized    neg    neu  \\\n",
       "0  FACE WITH TEARS OF JOY  Emoticons  :face_with_tears_of_joy:  0.247  0.285   \n",
       "\n",
       "     pos emoji  compound  \n",
       "0  0.468     😂     0.221  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦀 crab 0.0\n",
      "🙃 upside-down face 0.0\n",
      "🙏🏻 folded hands light skin tone 0.0\n",
      "🤢 nauseated face 0.0\n",
      "🤪 zany face 0.0\n",
      "✌🏼 victory hand medium-light skin tone 0.4939\n",
      "\n",
      "Size of X train:  531699\n",
      "\n",
      "Baseline Accuracy - y: 0.515\n",
      "\n",
      "Baseline Accuracy - y test: 0.517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    515\n",
       "1    485\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for transformer: \"TfidfVectorizer\" & estimator: \"LogisticRegression\"\n",
      "\tValidation Score:  0.963\n",
      "\tTrain Score:  1.0\n",
      "\tTest Score:  0.983\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter max_features for estimator ColumnTransformer(transformers=[('tfidfvectorizer',\n                                 TfidfVectorizer(ngram_range=(1, 2),\n                                                 stop_words=['beside', 'him',\n                                                             'due', 'ever',\n                                                             'wherein',\n                                                             'everyone', 'so',\n                                                             'after',\n                                                             'anywhere',\n                                                             'becomes',\n                                                             'however', 'whose',\n                                                             'cry',\n                                                             'thereafter',\n                                                             'often', 'thence',\n                                                             'call', 'please',\n                                                             'hereafter', 'on',\n                                                             'only', 'before',\n                                                             'made', 'around',\n                                                             'by', 'have', 'me',\n                                                             'whereafter',\n                                                             'latterly',\n                                                             'could', ...]),\n                                 'clean_text'),\n                                ('nosentimentizer', NoSentimentizer(),\n                                 'clean_text')]). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/azin/opt/anaconda3/envs/ga/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"/Users/azin/opt/anaconda3/envs/ga/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/Users/azin/opt/anaconda3/envs/ga/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/azin/opt/anaconda3/envs/ga/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/Users/azin/opt/anaconda3/envs/ga/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/Users/azin/opt/anaconda3/envs/ga/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 520, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"/Users/azin/opt/anaconda3/envs/ga/lib/python3.8/site-packages/sklearn/pipeline.py\", line 141, in set_params\n    self._set_params('steps', **kwargs)\n  File \"/Users/azin/opt/anaconda3/envs/ga/lib/python3.8/site-packages/sklearn/utils/metaestimators.py\", line 53, in _set_params\n    super().set_params(**params)\n  File \"/Users/azin/opt/anaconda3/envs/ga/lib/python3.8/site-packages/sklearn/base.py\", line 261, in set_params\n    valid_params[key].set_params(**sub_params)\n  File \"/Users/azin/opt/anaconda3/envs/ga/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\", line 232, in set_params\n    self._set_params('_transformers', **kwargs)\n  File \"/Users/azin/opt/anaconda3/envs/ga/lib/python3.8/site-packages/sklearn/utils/metaestimators.py\", line 53, in _set_params\n    super().set_params(**params)\n  File \"/Users/azin/opt/anaconda3/envs/ga/lib/python3.8/site-packages/sklearn/base.py\", line 249, in set_params\n    raise ValueError('Invalid parameter %s for estimator %s. '\nValueError: Invalid parameter max_features for estimator ColumnTransformer(transformers=[('tfidfvectorizer',\n                                 TfidfVectorizer(ngram_range=(1, 2),\n                                                 stop_words=['beside', 'him',\n                                                             'due', 'ever',\n                                                             'wherein',\n                                                             'everyone', 'so',\n                                                             'after',\n                                                             'anywhere',\n                                                             'becomes',\n                                                             'however', 'whose',\n                                                             'cry',\n                                                             'thereafter',\n                                                             'often', 'thence',\n                                                             'call', 'please',\n                                                             'hereafter', 'on',\n                                                             'only', 'before',\n                                                             'made', 'around',\n                                                             'by', 'have', 'me',\n                                                             'whereafter',\n                                                             'latterly',\n                                                             'could', ...]),\n                                 'clean_text'),\n                                ('nosentimentizer', NoSentimentizer(),\n                                 'clean_text')]). Check the list of available parameters with `estimator.get_params().keys()`.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pb/ky8pp8jx47lg6j6gkt8q6py80000gn/T/ipykernel_10845/1372365970.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_eda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/pb/ky8pp8jx47lg6j6gkt8q6py80000gn/T/ipykernel_10845/4016303222.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, to_model, run_eda)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LogisticRegression'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'TfidfVectorizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0minfo_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/pb/ky8pp8jx47lg6j6gkt8q6py80000gn/T/ipykernel_10845/4016303222.py\u001b[0m in \u001b[0;36mmodeling\u001b[0;34m(self, estimator, transformer, sentimentizer, grid_search, cv)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0mgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0mmdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0mto_save_mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/pb/ky8pp8jx47lg6j6gkt8q6py80000gn/T/ipykernel_10845/4016303222.py\u001b[0m in \u001b[0;36mdo_grid_search\u001b[0;34m(self, mdl, param_grid, cv, results)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m# Fit GridSearch to training data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;31m# What's the best score?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ga/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ga/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ga/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ga/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ga/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ga/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ga/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ga/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ga/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter max_features for estimator ColumnTransformer(transformers=[('tfidfvectorizer',\n                                 TfidfVectorizer(ngram_range=(1, 2),\n                                                 stop_words=['beside', 'him',\n                                                             'due', 'ever',\n                                                             'wherein',\n                                                             'everyone', 'so',\n                                                             'after',\n                                                             'anywhere',\n                                                             'becomes',\n                                                             'however', 'whose',\n                                                             'cry',\n                                                             'thereafter',\n                                                             'often', 'thence',\n                                                             'call', 'please',\n                                                             'hereafter', 'on',\n                                                             'only', 'before',\n                                                             'made', 'around',\n                                                             'by', 'have', 'me',\n                                                             'whereafter',\n                                                             'latterly',\n                                                             'could', ...]),\n                                 'clean_text'),\n                                ('nosentimentizer', NoSentimentizer(),\n                                 'clean_text')]). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "G = Main(to_model=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
