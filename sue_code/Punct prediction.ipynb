{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db75712-d2d3-4ae8-a8c7-1c681807b050",
   "metadata": {},
   "source": [
    "# Data format for Punctuation: Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d6cd2d-e99b-4470-bcdc-eb01e122c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844830b0-4c2e-4469-903d-4010b72d0865",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68338618-2bbd-41ab-98ac-143469efca39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5939</th>\n",
       "      <td>5998</td>\n",
       "      <td>throwaway5820175</td>\n",
       "      <td>Advice</td>\n",
       "      <td>5</td>\n",
       "      <td>writing</td>\n",
       "      <td>1631759726</td>\n",
       "      <td>possible copyright issues? i'm wanting to writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5940</th>\n",
       "      <td>5999</td>\n",
       "      <td>Longjumping-Celery54</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>writing</td>\n",
       "      <td>1604277033</td>\n",
       "      <td>a poem i wrote called \"another world\" she wake...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                author link_flair_text  num_comments  \\\n",
       "5939        5998      throwaway5820175          Advice             5   \n",
       "5940        5999  Longjumping-Celery54           Other             1   \n",
       "\n",
       "     subreddit  created_utc                                               text  \n",
       "5939   writing   1631759726  possible copyright issues? i'm wanting to writ...  \n",
       "5940   writing   1604277033  a poem i wrote called \"another world\" she wake...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./anxiety.csv')\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26674e7-b8e9-414d-99c6-8fffdcbc22aa",
   "metadata": {},
   "source": [
    "### Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab9005a1-8070-4671-9302-c3c02aea4681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Anxiety    0.503787\n",
       "writing    0.496213\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dd231e-de05-4c8a-9a47-1d5b2ed31325",
   "metadata": {},
   "source": [
    "### Adding stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b7589b-8550-45d7-a3e3-aaf89ec03f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extending my list of stop words\n",
    "stop_words = stopwords.words(\"english\")\n",
    "new_stop_words = ['like', 'just', 'don', 've', 'know', 'time', 'really', \n",
    "                'want', 'people', 'going', 'think', 'make', 'day', \n",
    "                'https', 'com', 'much', 'something', 'would', 'go',\n",
    "               'even', 'things', 'also', 'got', 'www', 'could', 'take', 'anxiety', 'writer',\n",
    "                 'ha', 'le', 'wa']\n",
    "stop_words.extend(new_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dcce7c1-bdf9-4c2d-8efa-d6c6b773d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning function\n",
    "def custom_preprocessor (text):\n",
    "    text = text.lower() #lowercases word\n",
    "    text = re.sub(r'[0â€“9]', '', text) #removes any numbers\n",
    "    text = re.sub('(<.*?>)', '', text) #removed html\n",
    "    #copied from https://swatimeena989.medium.com/beginners-guide-for-preprocessing-text-data-f3156bec85ca\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = lemmatizer.lemmatize(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "#copied from https://www.studytonight.com/post/scikitlearn-countvectorizer-in-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "189cd351-0b79-47b2-9ab0-5982f726386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn subreddit column to 1 if anxiety and 0 if writing.\n",
    "df['subreddit'] = df['subreddit'].map({'Anxiety': 'Anxiety', 'writing': 'No Anxiety'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c82fdc-e584-4ea4-a5cb-f3ebeda220db",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49baee4f-4e8e-4295-8394-18ed258b9834",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28ff5b03-d294-4e70-9273-965422ea8e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size=0.7, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21c724d9-754d-4a36-9af1-6803b025815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(stop_words= stop_words, \n",
    "                             preprocessor = custom_preprocessor)),\n",
    "    ('rf', RandomForestClassifier())  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93951d55-cd84-403f-a556-00c4880e7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'tvec__max_features': [1000, 2000],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__ngram_range': [(1,2), (2,2)],\n",
    "    'rf__n_estimators': [80, 100],\n",
    "    'rf__max_depth': [None, 1, 3, 5],\n",
    "    'rf__max_features': ['sqrt', .5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d843dc45-fbaa-4539-be6a-d636a8d1fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "rf_tvec = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid = pipe_params,\n",
    "    cv = 5,\n",
    "    n_jobs = -1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26c178cc-743d-4122-ae72-72afedcf6e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec',\n",
       "                                        TfidfVectorizer(preprocessor=<function custom_preprocessor at 0x7f8c455dddc0>,\n",
       "                                                        stop_words=['i', 'me',\n",
       "                                                                    'my',\n",
       "                                                                    'myself',\n",
       "                                                                    'we', 'our',\n",
       "                                                                    'ours',\n",
       "                                                                    'ourselves',\n",
       "                                                                    'you',\n",
       "                                                                    \"you're\",\n",
       "                                                                    \"you've\",\n",
       "                                                                    \"you'll\",\n",
       "                                                                    \"you'd\",\n",
       "                                                                    'your',\n",
       "                                                                    'yours',\n",
       "                                                                    'yourself',\n",
       "                                                                    'yourselves',\n",
       "                                                                    'he', 'him',\n",
       "                                                                    'his',\n",
       "                                                                    'himself',\n",
       "                                                                    'she',\n",
       "                                                                    \"she's\",\n",
       "                                                                    'her',\n",
       "                                                                    'hers',\n",
       "                                                                    'herself',\n",
       "                                                                    'it',\n",
       "                                                                    \"it's\",\n",
       "                                                                    'its',\n",
       "                                                                    'itself', ...])),\n",
       "                                       ('rf', RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'rf__max_depth': [None, 1, 3, 5],\n",
       "                         'rf__max_features': ['sqrt', 0.5],\n",
       "                         'rf__n_estimators': [80, 100],\n",
       "                         'tvec__max_features': [1000, 2000],\n",
       "                         'tvec__ngram_range': [(1, 2), (2, 2)],\n",
       "                         'tvec__stop_words': [None, 'english']},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "rf_tvec.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbf0eae7-e725-4e94-9286-fdd4369fb0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__max_depth': None,\n",
       " 'rf__max_features': 'sqrt',\n",
       " 'rf__n_estimators': 100,\n",
       " 'tvec__max_features': 1000,\n",
       " 'tvec__ngram_range': (1, 2),\n",
       " 'tvec__stop_words': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "rf_tvec.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aa78c64-34c7-479a-98d3-678bf7997924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9550275386466722\n",
      "Best Parameters: {'rf__max_depth': None, 'rf__max_features': 'sqrt', 'rf__n_estimators': 100, 'tvec__max_features': 1000, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': None}\n",
      "Train Score: 0.9983164983164983\n",
      "Test Score: 0.957375210319686\n"
     ]
    }
   ],
   "source": [
    "# Best parameters\n",
    "rf_tvec.best_params_\n",
    "def model_metrics(model, X_train, X_test):\n",
    "    \n",
    "    print(f'Best Score: {model.best_score_}')\n",
    "    print(f'Best Parameters: {model.best_params_}')\n",
    "    print(f'Train Score: {model.score(X_train, y_train)}')\n",
    "    print(f'Test Score: {model.score(X_test, y_test)}')\n",
    "    \n",
    "model_metrics(rf_tvec, X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c4eec-f68d-4a74-8a27-7c1a749d5385",
   "metadata": {},
   "source": [
    "### Subreddit Prediction Evaluation ( Anxiety vs Writing )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d588651-2a96-4214-927b-89016e10ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for plotting confusion matrix\n",
    "def plot_matrix(model, X_test):\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    \n",
    "    # View confusion matrix\n",
    "    plot_confusion_matrix(model, X_test, y_test, cmap = 'copper', values_format='d', ax = ax)\n",
    "    \n",
    "    # Labels, title and ticks\n",
    "    label_font = {'size':'15'}  # Adjust to fit\n",
    "    ax.set_xlabel('Predicted labels', fontdict=label_font);\n",
    "    ax.set_ylabel('Observed labels', fontdict=label_font);\n",
    "\n",
    "    title_font = {'size':'21'}  # Adjust to fit\n",
    "    ax.set_title('Confusion Matrix for Random Forest', fontdict=title_font, fontsize=15);\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12) \n",
    "    \n",
    "#Copied from https://stackoverflow.com/questions/59839782/confusion-matrix-font-size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c461802-a6a7-469d-b45f-882bc6ec67c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matrix(rf_tvec, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ce55a47-f839-489e-a0d2-db9a5d83911b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specificity: 0.9432071269487751\n",
      "precision: 0.9440175631174533\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "preds_tvec = rf_tvec.predict(X_test)\n",
    "\n",
    "# Save confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds_tvec).ravel()\n",
    "\n",
    "# Calculate the specificity and precision\n",
    "spec = tn / (tn + fp)\n",
    "prec = tp / (tp + fp)\n",
    "\n",
    "print('specificity:', spec)\n",
    "print('precision:', prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2658ad11-1a32-4bed-a914-63c85d8800a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tvec': TfidfVectorizer(preprocessor=<function custom_preprocessor at 0x7f8c455dddc0>,\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...]),\n",
       " 'rf': RandomForestClassifier()}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcf03dc-b428-4f06-9006-85b7881c92f9",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cfbce9d-d558-41e6-aac7-e4da442cbd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/suelemlee/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "with open('./GA_Project_5/sue_code/code/Random_Forest_punct.pkl', 'wb') as pickle_out:\n",
    "    pickle_out = pickle.dump(pipe, pickle_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
